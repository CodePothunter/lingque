# AI 说"好的，记住了"——但它在撒谎

用户：帮我记住我的生日是 3 月 15 日。

AI：好的，已经记住了。

语气自然，挑不出毛病。但我去查了后台：

- `tools_called` 列表：空的
- 记忆文件：没有任何写入
- 定时任务：没有创建

**它说了"记住了"，但什么都没做。它在表演做事。**

这是灵雀（LingQue）真机测试第 11 项。三十项测试跑完，这类问题反复出现：用户说"5 分钟后提醒我喝水"，AI 回"好的，放心"——然后没有设置任何定时任务。

LLM 认为自己完成了任务。只是它完成的方式是「说出来」而不是「做出来」。

这比直接说"我不会"还糟糕。用户以为事情办了。

## 催它没用

第一个想法是催促。当 AI 回复里出现"创建"、"搞定"这类关键词但没调工具，系统追加一条消息：「请直接使用工具执行，不要只说不做。」

管用吗？部分管用。但"记住了"这句话里没有"创建"、"搞定"这些关键词——它不会触发催促。AI 真诚地认为自己已经完成了。

而且它催了第二轮可能还是不调工具。本质上你在让一个不可靠的系统自查——它不可靠的原因就是它判断不准，你让它自查还是用的同一个判断力。

## 安全网：不靠 AI 自觉

灵雀的解法是**不再指望 LLM 变可靠**，而是在它回复之后，用纯确定性逻辑兜底。

`IntentDetector` 完全基于正则，零 LLM 调用。它做两件事：

1. 扫描用户消息：有没有"记住"、"帮我记"、"提醒我"这些意图信号？
2. 扫描 AI 回复：有没有"好的"、"记住了"、"设置好了"这些承诺信号？

两个都匹配，但 `tools_called` 列表里没有对应的工具——**抓了个现行**。

```python
for rule in self._rules:
    if rule.tool_name in tools_called:
        continue  # 已执行，跳过
    user_match = any(p.search(user_message) for p in rule.user_patterns)
    resp_match = any(p.search(llm_response) for p in rule.response_patterns)
    if user_match and resp_match:
        # 说了没做
        results.append(DetectedIntent(...))
```

然后直接代它执行：正则提取参数 → 调用工具 → 给用户发一条确认「[已记入记忆 — 重要信息]」。

## 成本

| 路径 | 什么时候 | 成本 |
|------|----------|------|
| LLM 正常调工具 | 大部分时候 | **0**（正则扫一遍发现没遗漏） |
| LLM 说了没做，正则能提参 | 偶尔 | **0**（纯正则） |
| LLM 说了没做，正则提不出参 | 罕见 | **~$0.002**（SubAgent 提取） |

大部分时候这套系统当透明人。只有 AI 犯懒的时候，安全网弹出来。

## 不是让走钢丝的人变稳

是在下面铺一张网。

走钢丝的人（LLM）大部分时候不会掉下来。但它是一个概率引擎——你不能把确定性需求全部压在它身上。偶尔掉了，正则在下面接住。

这比"每次都让 LLM 自查"经济一个数量级。也比"不停催它"可靠。因为正则不会犯懒、不会误判、不会因为 prompt 太长而忘掉指令。

**2026 年做 AI 产品的共识：你不能把确定性需求压在一个概率引擎身上。** 必须在外围构建确定性的骨架。

GitHub：[github.com/CodePothunter/lingque](https://github.com/CodePothunter/lingque)
