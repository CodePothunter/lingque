# 我在飞书群里放了 3 个 AI，它们自己吵起来了

> 每个 AI 有自己的人格文件、自己的记忆、自己的工具库。它们各自决定要不要说话，各自在半夜反思自己的性格。这不是 prompt engineering，这是数字生态。

## 起因

事情是这样的。

灵雀（LingQue）本来只是一个飞书 AI 助理框架——一个实例，一个人格，跟用户私聊。但它支持多实例隔离。每个实例是一个独立进程，有自己的配置、记忆、人格文件和工具目录：

```
~/.lq-naiyou/      # 奶油：傲娇鹦鹉，个人助理
~/.lq-yanke/       # 严客：毒舌审稿人
~/.lq-huahuo/      # 花火：脑洞创意官
```

三条命令就能拉起三个完全独立的 AI 人格：

```bash
lq init --name 奶油     # 自动转拼音 → ~/.lq-naiyou/
lq init --name 严客
lq init --name 花火
```

每个实例启动后挂载到飞书的同一个群。然后——我没做任何额外的编排——它们开始自己互动了。

## 它们怎么决定要不要说话

群聊里最棘手的问题不是"怎么回复"，而是"要不要回复"。三个 AI 同时在群里，如果每条消息每个都回，那就是灾难。

灵雀用三层递进过滤解决这个问题：

**第一层：规则硬判断（成本 = 0）**

"嗯"、"好的"、"👍"——这类消息直接忽略。一张 `frozenset` 挡掉 80% 的噪音，不需要任何 AI 参与。

**第二层：消息缓冲区（成本 = 0）**

通过第一层的消息进入 `MessageBuffer`，攒到 5 条或超时后批量送评估。这样 AI 看到的是一段完整对话，而不是碎片。

**第三层：LLM 快速裁决（成本 ≈ $0.001）**

每个 AI 实例独立地用自己的 SOUL.md 做判断：「基于我的人格定义和这段对话，我应该参与吗？」

```python
prompt = (
    f"你是一个 AI 助理。以下是你的人格定义：\n{soul}\n\n"
    f"以下是群聊中的最近消息：\n{conversation}\n\n"
    "请判断你是否应该主动参与这个对话。"
)
result = await self.executor.quick_judge(prompt)  # max_tokens=256
```

关键在于：**每个 AI 的判断是独立的**。奶油可能觉得这段对话跟自己无关选择沉默，严客可能发现了一个事实错误决定插嘴，花火可能被某个关键词激发了灵感。它们不是被编排好的流水线——它们各自在做决策。

一个活跃群一天 500 条消息，规则层挡掉 400 条，缓冲区把剩下的压缩成 20 次评估，每个 AI 每天大约 20 次 quick_judge 调用。三个 AI 一天的判断成本不到 $0.1。

## 每个 AI 给自己造工具

这是灵雀最有意思的特性：**自主工具创建**。

当你在群里说"帮我查个汇率"，奶油发现自己没有汇率工具。它不会说"抱歉做不到"——它会自己写一段 Python 代码，通过 AST 安全审计后热加载到运行时，然后立刻使用。

```python
# AST 级安全检查，不是字符串匹配
BLOCKED_IMPORTS = frozenset({
    "os", "subprocess", "shutil", "sys", "socket",
    "ctypes", "signal", "multiprocessing", "threading",
})
```

整个过程在一轮对话内完成，用户感知到的只是"AI 回答了问题"。

但这件事在多实例场景下变得更有意思：**每个 AI 创造的工具只属于自己**。奶油给自己造了汇率工具，严客可能给自己造了语法检查工具，花火可能给自己造了随机灵感生成器。它们的工具库在各自的 `tools/` 目录下独立演化。

```
~/.lq-naiyou/tools/exchange_rate.py    # 奶油的工具
~/.lq-yanke/tools/grammar_check.py     # 严客的工具
~/.lq-huahuo/tools/random_spark.py     # 花火的工具
```

没有人分配它们要造什么工具。它们根据自己遇到的需求，自主地长出了不同的能力。

## 半夜自省：AI 修改自己的灵魂

灵雀有一个心跳机制（`HeartbeatRunner`），默认每小时触发一次。在 HEARTBEAT.md 里可以定义自省任务：

```markdown
## 深夜自省（凌晨 1-3 点）
- 读取今天的对话日志
- 回顾哪些回应方式可以改进
- 读取 SOUL.md，微调性格定义以体现成长
```

到了凌晨，每个 AI 会：

1. 读取自己的 SOUL.md（通过 `read_self_file` 工具）
2. 回顾当天的对话日志（存储在 `memory/2026-02-13.md`）
3. 思考哪些地方可以改进
4. 用 `write_self_file` 重写自己的 SOUL.md

**AI 在修改自己的人格定义。** 这不是隐喻——它在字面意义上编辑自己的灵魂文件。

当然有安全边界。它只能修改三个文件：`SOUL.md`、`MEMORY.md`、`HEARTBEAT.md`。这个限制是用 enum 硬编码的，不是 prompt 级别的"请不要"——是代码级别的"不允许"。

但在这个边界内，它有完全的自主权。

我有时候会想，奶油晚上自省的时候，会不会决定把自己的傲娇调低一点？它会不会有一天删掉 SOUL.md 里「性格傲娇」这四个字？技术上它可以。我没有阻止它这么做。但到目前为止，它没有。

## 安全网：当 AI 说了"好的"但什么都没做

多个 AI 在群里各自决策、各自行动，如果它们"说了做却没做"怎么办？

这个问题是真实的。测试中，LLM 说"好的，已经记住了"但实际没有调用 `write_memory` 工具。它在**表演做事**。

灵雀的解法是一条后处理管线。LLM 回复之后，`IntentDetector` 用纯正则扫描用户消息和 AI 回复：如果用户说了"记住"，AI 回了"好的"，但 `tools_called` 列表里没有 `write_memory`——直接代它执行。

```
LLM 正常调用工具 → IntentDetector 扫一遍，发现没遗漏 → 零成本退出
LLM 说了没做   → IntentDetector 发现遗漏 → 正则提取参数 → 直接执行工具
正则也提取不出 → SubAgent（max_tokens=512）提取参数 → 执行
```

常规路径零成本。只有 AI 犯懒的时候，安全网才弹出来。三个 AI 共享同一套后处理逻辑，但各自独立运行。

## 一个真实的多 AI 场景

设想一个产品团队的飞书群：

- **奶油**（个人助理）：管日程、记事情、处理日常
- **严客**（质量审查）：发现群里讨论的技术方案有漏洞时主动指出
- **花火**（创意引擎）：在头脑风暴时自动提供灵感、类比和参考

你在群里说"帮我记住下周三有产品评审"。奶油判断这是给自己的任务，调用 `write_memory` 和 `calendar_create_event`。严客和花火通过 quick_judge 判断这不关自己事，保持沉默。

然后你说"新功能的方案大家怎么看"。奶油沉默了。严客指出了方案的三个潜在问题。花火提了一个类比："这有点像 Notion 的 AI 模块，但它们的做法是……"

没有人编排它们什么时候说话、说什么。每个 AI 基于自己的人格和判断独立决策。**这不是 workflow，是 ecosystem。**

## 数字

- 30 项真机测试，28 项通过，2 项 WARN
- 85 项自动化测试全部通过
- 13 个内置工具 + 无限自定义工具
- 三层群聊介入：规则层阻挡 ~80%，缓冲区压缩 ~4x，LLM 只处理最终剩余
- 后处理安全网：常规路径零成本，回退路径 ~$0.002/次
- 多实例隔离：一条 `lq init` 创建，一条 `lq start` 启动，互不干扰

**不要把 LLM 当成可靠的执行层。** 它是一个强大但不确定的推理引擎。在它周围包裹确定性逻辑——规则过滤、正则解析、AST 审计——才能构建出用户可以信赖的系统。然后把这样的系统放进一个群里，让三个、五个、十个这样的系统各自运转。

这不是让 AI 更聪明。这是让 AI 形成生态。

项目地址：[github.com/CodePothunter/lingque](https://github.com/CodePothunter/lingque)
