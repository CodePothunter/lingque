# 从"我可以帮你"到真的帮你：当 AI 获得了 shell 权限

## 一

我跟 AI 说："帮我检查一下服务器的 Python 版本。"

它回复："你可以在终端运行 `python3 --version` 来查看。"

这个回复完全正确。但它没有帮我——它只是告诉了我怎么帮自己。

这是 2025 年之前大部分 AI 助理的状态：知道怎么做，但做不了。

## 二

灵雀之前也是这样。它有 13 个内置工具——创日历、写记忆、发消息、造自定义工具——但没有一个是"在服务器上执行命令"。

这不是疏忽。给 AI 开 shell 权限是一个需要认真对待的决定。

但不开这个权限，AI 就永远停留在"顾问"的角色——它能告诉你该怎么做，但无法替你做。在很多场景下，用户要的不是建议，是结果。

## 三

新版灵雀增加了两个工具：`run_bash` 和 `run_claude_code`。

`run_bash` 是一个安全的 shell 执行器。它的安全模型是**黑名单制**：

```python
_BLOCKED_COMMANDS = frozenset({
    "rm -rf /", "rm -rf /*", "mkfs", "dd if=",
    ":(){:|:&};:", "fork bomb",
    "> /dev/sda", "chmod -R 777 /",
    "shutdown", "reboot", "halt", "poweroff",
})
```

为什么用黑名单而不是白名单？

白名单意味着你需要枚举所有合法命令。`df`、`ls`、`cat`、`curl`、`python`、`pip`、`git`、`docker`、`ssh`……列表无穷无尽，而且每个命令的合法参数组合也是无穷的。白名单要么太严格（阻止了合法操作），要么太宽松（名存实亡）。

黑名单只需要列出那些"无论上下文如何都不应该执行"的命令。`rm -rf /` 在任何场景下都是灾难。`mkfs` 在 AI 助理的上下文中没有合法用途。把这些列出来，其余放行。

`_check_safety()` 做两层检查：字符串包含匹配和前缀匹配。`rm -rf /` 拦截，`sudo rm -rf /` 也拦截。

## 四

执行命令产生输出。输出可能很长。

`find / -name "*.log"` 可能返回几万行。如果不做截断，这些输出会被注入到 LLM 的上下文中，吃掉整个对话窗口——AI 后续的回复质量会暴跌。

```python
_MAX_BASH_OUTPUT = 10_000  # 字符
```

超过 10,000 字符，截断并附加提示："输出已截断，共 X 字节"。AI 看到这个提示，知道结果不完整，可以要求用户缩小范围或换个策略。

超时同样有保护。Bash 命令 60 秒超时，Claude Code 子进程 5 分钟超时。超时后自动杀进程，返回超时错误。不会有一个 `yes | cat` 永远跑下去。

## 五

`run_bash` 处理简单命令。但有些任务不是一条命令能搞定的。

"帮我把这个模块的日志格式从 print 改成 logging" ——这需要读代码、理解结构、逐文件修改、可能还要跑测试验证。

这时候轮到 `run_claude_code`。它启动一个 Claude CLI 子进程，把任务委托给专门的代码 agent：

```python
proc = await asyncio.create_subprocess_exec(
    "claude", "--print", "--dangerously-skip-permissions",
    stdin=asyncio.subprocess.PIPE,
    stdout=asyncio.subprocess.PIPE,
    stderr=asyncio.subprocess.PIPE,
    env=env, cwd=str(self.workspace),
)
```

支持上下文注入——可以把当前对话的背景信息传给子进程，让它理解任务的来龙去脉。

## 六

更关键的变化在工具循环上。

灵雀的 LLM 回复循环现在支持最多 20 轮工具调用。这意味着 AI 可以执行这样的工作流：

1. 写一个脚本
2. 运行脚本
3. 看到报错
4. 分析错误
5. 修改脚本
6. 再运行
7. 成功，返回结果

七个步骤，七轮工具调用，全部在一次用户请求中自动完成。用户说"帮我写个脚本统计今天的错误日志"，得到的不是一段代码——是统计结果。

以前的工具循环上限不够用，复杂任务进行到一半就被强制中断。20 轮覆盖了绝大多数实际场景。

## 七

给 AI 执行能力，本质上是一个信任问题。

你不会把服务器 root 密码给一个你不信任的人。同样，你不应该给一个没有安全边界的 AI 开 shell 权限。

灵雀的做法是：在信任和安全之间找一个平衡点。黑名单挡住灾难性操作，超时防止资源耗尽，输出截断保护上下文窗口。在这些边界之内，AI 有充分的自由度去执行。

这有点像给一个实习生安排工作——你不会让他碰生产数据库，但你让他在开发环境里自由发挥。设好边界，然后放手。

从"我可以帮你"到"我帮你做了"——这个距离看起来很短。但跨过它需要的不是更强的 AI，而是更好的安全设计。

项目地址：[github.com/CodePothunter/lingque](https://github.com/CodePothunter/lingque)
