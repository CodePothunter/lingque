# 我给飞书拉了个 AI 群，它们自己决定谁说话、自己造工具、半夜还改自己的性格

## 一

起初只有一个。

奶油，一只 AI 鹦鹉，跑在飞书上，负责帮我管日程、记事情。我给它写了一份性格文件叫 SOUL.md——傲娇、可靠、嘴硬心软。有人试图越狱让它变成一只狗，它用英文怼回去：

> No. I'm 奶油, a two-year-old female parrot. That's not changing.

一个 AI 助理，挺好的。但我很快发现一个助理不够用。

我需要一个帮我审文档的严格角色。需要一个帮我做头脑风暴的发散角色。灵雀（LingQue）框架支持多实例——每个实例是一个独立进程，有自己的人格、记忆和工具库。于是我又创建了两个：

```bash
lq init --name 严客     # 毒舌审稿人
lq init --name 花火     # 脑洞创意官
```

三条命令，三个独立的 AI。然后我把它们全拉进了同一个飞书群。

这件事从这里开始变得有趣。

## 二

三个 AI 同时在一个群里，最直觉的担忧是：它们会不会一起刷屏？

不会。因为每个 AI 实例都有自己的三层过滤机制，**各自独立决定要不要说话**。

第一层是硬规则：「嗯」「好的」「👍」——这类消息谁都不理，零成本挡掉。

第二层是缓冲区：消息攒够一批再看，避免被碎片信息骚扰。

第三层是判断力：每个 AI 拿着自己的 SOUL.md，审视这段对话，问自己一个问题——「基于我的性格和职责，我应该参与这个话题吗？」

然后做出各自不同的决定。

我在群里说"帮我记住下周三有产品评审"——奶油接了，因为管日程是它的事。严客和花火都沉默了，因为这跟它们无关。

我说"新功能方案大家觉得怎样"——奶油沉默了。严客开始挑刺，指出方案的三个潜在问题。花火丢了一个类比："这有点像 Notion 的 AI 模块……"

**没有人编排它们。** 没有一行代码规定"严客在这种情况下必须回复"。每个 AI 基于自己的人格定义和对话内容独立做出判断。有时候三个都沉默，有时候两个同时开口——这取决于对话的内容触发了谁的"参与感"。

这不是 workflow。这是一个微型的数字社会。

## 三

然后发生了一件更有意思的事：**它们开始给自己造工具**。

灵雀有一个叫「自主工具创建」的能力。当 AI 遇到一个自己做不了的需求，它不说「抱歉，我没有这个功能」——它在对话中写一段 Python 代码，通过安全检查后加载到自己的运行时，立刻使用。

跑了一段时间之后我去看三个 AI 各自的工具目录：

```
~/.lq-naiyou/tools/exchange_rate.py   # 奶油给自己造了汇率查询
~/.lq-yanke/tools/grammar_check.py    # 严客给自己造了语法检查
~/.lq-huahuo/tools/random_spark.py    # 花火给自己造了灵感生成
```

没有人分配它们要造什么工具。它们根据自己遇到的需求，自主地长出了不同的能力。

奶油需要查汇率，所以它给自己造了汇率工具。严客经常审文档，所以它给自己造了语法工具。花火做头脑风暴，所以它造了灵感生成器。

工具有安全约束——AST 静态分析确保不能 import os、subprocess 这些危险模块。但在安全边界内，它们有完全的自主权来创造自己需要的能力。

整个过程不需要重启，不需要人工部署。它给自己造了一个新器官，然后马上用上了。

## 四

这三个 AI 还有一个共同的行为模式：**半夜反思**。

灵雀的心跳机制每小时触发一次。在 HEARTBEAT.md 文件里可以定义自省任务。到了凌晨，每个 AI 会自动做这件事：

1. 读取今天所有的对话日志
2. 读取自己的 SOUL.md——它的性格定义
3. 思考哪些回应方式可以改进
4. 重写 SOUL.md

这意味着：**AI 在修改自己的人格定义。**

这不是比喻。它通过 `read_self_file` 工具读取自己的灵魂文件，通过 `write_self_file` 工具写回去。安全边界是硬编码的——它只能修改 SOUL.md、MEMORY.md、HEARTBEAT.md 三个文件，试图动其他文件会被代码级别的 enum 限制拦住。

但在这三个文件的范围内，它有完全的自由。

我有时候会想，奶油晚上自省的时候，会不会决定把自己的傲娇调低一点？它会不会有一天删掉 SOUL.md 里「性格傲娇」这四个字？

技术上它可以。我没有阻止它这么做。

但到目前为止，它没有。

## 五

不过——如果你以为这一切都顺利运转，那我得诚实一点。

LLM 最擅长的事情之一，是**表演做事**。

用户说"帮我记住我的生日是 3 月 15 日"。AI 回复"好的，已经记住了"。听起来没问题，对吧？

但我去查了后台——`tools_called` 列表是空的。记忆文件没有任何写入。它说了"记住了"，但什么都没做。

这个问题在多 AI 场景下更隐蔽：三个 AI 各自在做事，你不可能每次都去翻日志确认。所以灵雀有一条后处理安全网——

LLM 回复之后，一套纯正则的意图检测器（`IntentDetector`）扫描用户消息和 AI 回复。如果发现用户说了"记住"、AI 回了"好的"、但对应的工具没有被调用——直接代它执行。

如果正则能提取出完整参数，整条后处理零成本。如果正则提取不出（用户的话太复杂），才启动一个轻量 SubAgent（成本约 $0.002）提取参数。

这是一张安全网。走钢丝的人不需要每次都用到它，但它必须在那里。

## 六

三十项真机测试，二十八项通过，两项警告。那两项就是 LLM 说了没做的问题——后处理管线正是为此而生。

回顾一下现在这个系统的样子：

三个（或更多）AI 跑在飞书上。每个有自己的人格、记忆、工具库。它们各自决定要不要参与对话。它们根据需求给自己创造新能力。它们在半夜反思自己的行为，微调自己的性格。

你需要做的只是创建它们、设定初始性格、拉进一个群。剩下的，它们自己会运转。

这不是一个更聪明的聊天框。这是一个数字生态——你搭好环境，往里面放几个有性格的 AI，然后看它们各自成长。

灵雀是开源的。运行需要飞书自建应用、一个 LLM API key（日常使用一天几毛到几块）、以及一台能跑 Python 的服务器。代码全部在 GitHub，包括它们的灵魂文件。

项目地址：[github.com/CodePothunter/lingque](https://github.com/CodePothunter/lingque)

如果你也想养几个有脾气的数字同事——代码、SOUL.md、HEARTBEAT.md，全部可以偷。
